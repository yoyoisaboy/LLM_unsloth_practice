{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM ?\n",
    "* LLM = Large Language Model，大型語言模型，用來處理和生成自然語言。\n",
    "* 架構：基於 [Transformer](https://medium.com/@glowing_sage_deer_60/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021-transformer-533006e3474) 生成。\n",
    "* 輸入：文字（Token）；輸出：預測下一個 Token，直到結束。\n",
    "\n",
    "## 生成式AI概念\n",
    "在生成式AI開始流行前，受限於硬體與資料量，人們大部分只能使用有限的資源，用監督式的方式進行訓練(CNN、LSTM...)，而強化式學習的方法(GAN)只可能出現在論文的研究上，比較少看到實務上使用。大概2023年末開始，Google 團隊 OpenAI 開始起步線上開放給大家使用 chatGPT 後，大家開始發現強化式學習的厲害之處。\n",
    "\n",
    "生成式AI訓練的方式，簡單用小偷作假鈔的例子當作範例:\n",
    "\n",
    "<img src=\".\\img\\img1.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "## 主流模型有哪些 (~2025)\n",
    "| 模型 | 類型 | 開源 | 訓練參數量 | 運作方式 | 備註 |\n",
    "| -------- | -------- | -------- |-------- | -------- | -------- |\n",
    "| GPT-4     | API     | 否     |未公開     | 雲端推論     | 支援工具使用 |\n",
    "| Claude 3     | API     | 否     |未公開     | 雲端推論     | 可處理大量上下文 |\n",
    "| Gemini     | API     | 否     |未公開     | 雲端推論     | 整合 Google 生態|\n",
    "| LLaMA 3    |開源     | 是     |8B / 70B     | 本地 / 雲皆可    | 授權較嚴格 |\n",
    "| Mistral     | 開源     | 是     |7B     | 支援 GGUF     | 效能優、體積小 |\n",
    "| Phi-2     | 開源    | 是     |2.7B     | 適合邊緣部署     | 微軟研究模型|\n",
    "| Gemma    | 開源    | 是     |\t2B / 7B     | 易於部署     | Google 出品 |\n",
    "\n",
    "## 要建立自己的LLM，需要哪些東西\n",
    "1. 確認目標\n",
    "* 沒想寫code，只想用線上版GPT : OpenAI API / Claude / Gemini (補充:[OpenAI微調範例](https://www.accucrazy.com/openai-fine-tuning/))\n",
    "* 想「部署模型」：選開源模型，考慮硬體資源\n",
    "* 想「訓練模型」：需要數百萬以上資料 + GPU 叢集\n",
    "2. 本地部署選項\n",
    "就是已經有了模型，只需要安裝幾個軟體即可\n",
    "* 工具選擇：llama.cpp（C++）、Ollama（簡單一行部署）、LM Studio（GUI）\n",
    "* 支援格式：GGUF（用於量化後模型）\n",
    "3. 推論條件\n",
    "* 至少需：16GB RAM + 6~8GB VRAM（Mistral/Gemma/phi 可行）\n",
    "* 高效部署工具：[vLLM](https://github.com/vllm-project/vllm)（商用級推論）、[Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index)（HuggingFace）\n",
    "4. 寫程式碼呼叫API\n",
    "* 所需資訊：API Key、安全性管理、輸入輸出格式（JSON）\n",
    "* 推薦套件：openai, anthropic, google.generativeai\n",
    "\n",
    "\n",
    "# unsloth\n",
    "github : https://github.com/unslothai/unsloth?tab=readme-ov-file\n",
    "* 速成的LLM，特別針對訓練速度和低資源環境進行了優化\n",
    "* 有寫好的很多開源模型做 Fine-tune 的程式碼，已經放在 Google Colab 上可以直接使用\n",
    "* 僅支援單卡運算\n",
    "\n",
    "\n",
    "# 基本環境準備\n",
    "1. Python 環境（建議 Python 3.10 或以上）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m venv llm_env\n",
    "# source llm_env/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CUDA & 驅動\n",
    "確保你的機器有 NVIDIA GPU，並安裝對應版本的：\n",
    "* CUDA：11.8 或 12.x（視你 GPU 支援而定）\n",
    "* cuDNN：與 CUDA 相符\n",
    "* [安裝教學](https://medium.com/ching-i/win10-安裝-cuda-cudnn-教學-c617b3b76deb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 25 17:23:06 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P0             13W /   90W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安裝套件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip\n",
    "#看cuda版本\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu127 \n",
    "# pip install unsloth\n",
    "# pip install transformers accelerate peft bitsandbytes datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下載模型\n",
    "Unsloth 支援的模型格式為 HuggingFace 格式，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,        # 自動選擇 torch.float16 or bfloat16\n",
    "    load_in_4bit = True  # 使用 bitsandbytes 量化模型\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例快速部署（聊天測試）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    prompt = input(\"You: \")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用範例資料集:\n",
    "[AWeirdDev/zh-tw-pts-articles-sm](https://huggingface.co/datasets/AWeirdDev/zh-tw-pts-articles-sm)->公視新聞網（news.pts.org.tw）所抓取的新聞資料。資料集共有 1,400 筆資料，每筆資料包含以下欄位：\n",
    "\n",
    "    - image：縮圖\n",
    "    - title：標題\n",
    "    - conclusion：結論（「結論先講」一欄，可能為 None）\n",
    "    - content：文章內容\n",
    "    - timestamp：發布時間\n",
    "    - category：類別（可能為 None）\n",
    "    - link：文章連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速確認格式：\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"AWeirdDev/zh-tw-pts-articles-sm\")\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unsloth 與資料集怎麼搭配用？\n",
    "假設你使用 LLaMA 3 8B 4bit 模型（Unsloth 最常見選項），並以 LoRA 做微調訓練："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 載入模型\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以根據任務設計 prompt，例如：\n",
    "\n",
    "Chat 模型（問答導向）：\n",
    "\n",
    "`Human: 以下是一篇文章，請幫我寫摘要：{text} \\nAssistant:`\n",
    "\n",
    "Summarization 模型（摘要導向）：\n",
    "\n",
    "`請針對以下內文產生摘要：{text} \\n摘要：`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 準備資料（轉成 SFT 格式），需要轉為-> {\"prompt\": \"...\", \"completion\": \"...\"}\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"AWeirdDev/zh-tw-pts-articles-sm\")\n",
    "\n",
    "def convert(example):\n",
    "    prompt = f\"請閱讀以下內文，並撰寫摘要：\\n{example['text']}\\n\\n摘要：\"\n",
    "    completion = example['summary']\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "dataset = raw_dataset[\"train\"].map(convert)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 套用 tokenizer 與訓練\n",
    "from unsloth import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# 使用 tokenizer 處理\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"prompt\"], x[\"completion\"]), remove_columns=dataset.column_names)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = True,\n",
    "        output_dir = \"./lora-output\",\n",
    "        save_total_limit = 2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 訓練完成後，你可以使用 model.save_pretrained() 或 peft_model.save_pretrained() 把 LoRA 存起來\n",
    "* 輸出路徑： ./lora-output/  # 內有 adapter_config.json, adapter_model.bin 等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一、載入微調後的模型\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 載入原始模型（base）\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# 套用微調的 LoRA 權重\n",
    "model = PeftModel.from_pretrained(model, \"./lora-output\", is_trainable=False)\n",
    "model.eval()  # 切成 eval 模式，避免 dropout 等訓練機制影響\n",
    "\n",
    "# 確保 tokenizer 也正確\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二、開始推論，這裡假設任務是「繁體中文文章摘要」：\n",
    "def summarize(text):\n",
    "    prompt = f\"請閱讀以下內文，並撰寫摘要：\\n{text}\\n\\n摘要：\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 移除 prompt\n",
    "    return result.split(\"摘要：\")[-1].strip()\n",
    "\n",
    "# 測試\n",
    "text = \"日本東京大學近日宣布，他們成功開發出一種能在低溫環境下穩定運作的量子晶片……\"\n",
    "print(\"摘要結果：\", summarize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 進階 : 打包成API\n",
    "\n",
    "如果你之後想要給網頁串接或當本地應用，可以用 Flask 快速包一個 API："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/summarize\", methods=[\"POST\"])\n",
    "def summarize_api():\n",
    "    data = request.get_json()\n",
    "    text = data.get(\"text\", \"\")\n",
    "    summary = summarize(text)\n",
    "    return jsonify({\"summary\": summary})\n",
    "\n",
    "app.run(port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
